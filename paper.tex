\documentclass[12pt]{article}

% General
\usepackage[round]{natbib}
\usepackage{setspace}
\usepackage{geometry}
\usepackage[section]{placeins}
\usepackage[hidelinks]{hyperref}
\usepackage{graphicx}
\usepackage{xcolor}
\usepackage{titlesec}
\usepackage[page]{appendix}
\usepackage{enumerate}

% Tables/Figures
\usepackage{lscape}
\usepackage{booktabs}
\usepackage{rotating}
\usepackage{multirow}
\usepackage{longtable}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{float}
\usepackage{tabularx}
\usepackage{ragged2e}
\newcolumntype{Y}{>{\RaggedRight\arraybackslash}X}
\usepackage{pdflscape}
\usepackage{afterpage}

% Math
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{mathtools}
\usepackage{dsfont}

\usepackage{tikz}
\usetikzlibrary{bayesnet}

% \doublespacing
\onehalfspacing
% \singlespacing

% \numberwithin{equation}{section}

\geometry{paper=letterpaper, margin=1in}
\captionsetup{font=small}

% Code
\usepackage{textcomp}
\usepackage{sourcecodepro}
\usepackage{listings}
\definecolor{commentgrey}{gray}{0.45}
\definecolor{backgray}{gray}{0.96}
\lstset{
  basicstyle=\footnotesize\ttfamily, keywordstyle=\footnotesize,
  backgroundcolor=\color{backgray}, commentstyle=\color{commentgrey},
  frame=single, rulecolor=\color{backgray}, showstringspaces=false,
  breakatwhitespace=true, breaklines=true, upquote=true,
  numbers=left, numberstyle=\footnotesize\color{commentgrey}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% User-defined LaTeX commands
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\Corr}{Corr}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclarePairedDelimiter{\abs}{\lvert}{\rvert}
\DeclarePairedDelimiter{\norm}{\lVert}{\rVert}
\newcommand*{\expp}[1]{\exp\left(#1\right)}
\newcommand*{\foralls}{\ \forall \ }
\newcommand*{\st}{\text{ s.t. }}
\newcommand*{\E}{\mathbb E}
\newcommand*{\R}{\mathbb R}
\newcommand*{\I}{\mathds{1}}
\newcommand*{\Prob}{\mathbb P}
\newcommand*{\convas}[1]{\xrightarrow{#1}}
\newcommand*{\conv}{\convas{}}
\newcommand*{\cond}{\;\ifnum\currentgrouptype=16 \middle\fi|\;}
\newcommand*{\defeq}{%
  \mathrel{\overset{\makebox[0pt]{\mbox{\normalfont\tiny\sffamily def}}}{=}}}
\newcommand*{\notorth}{\ensuremath{\perp\!\!\!\!\!\!\diagup\!\!\!\!\!\!\perp}}
\newcommand*{\orth}{\ensuremath{\perp\!\!\!\perp}}
\newcommand*{\evalat}{\,\big\rvert}
\newcommand*{\dif}{\,d}
\newcommand*{\difto}[1]{\,d^#1}
\newcommand*{\difbot}[1]{\frac{d}{d#1}}
\newcommand*{\partialbot}[1]{\frac{\partial}{\partial#1}}
\newcommand*{\m}[1]{\textbf{#1}}
\newcommand*{\bmath}[1]{\boldsymbol{#1}}

\newcommand*{\yestag}{\addtocounter{equation}{1}\tag{\theequation}}
\newcommand*{\notaligned}[1]{\noalign{$\displaystyle #1$}}
\newcommand*{\ttilde}{{\raise.17ex\hbox{$\scriptstyle\sim$}}}

\makeatletter
\newsavebox{\mybox}\newsavebox{\mysim}
\newcommand*{\distas}[1]{%
  \savebox{\mybox}{\hbox{\kern3pt$\scriptstyle#1$\kern3pt}}%
  \savebox{\mysim}{\hbox{$\sim$}}%
  \mathbin{\overset{#1}{\kern\z@\resizebox{\wd\mybox}{\ht\mysim}{$\sim$}}}%
}
\makeatother
\newcommand*{\dist}{\sim}
\newcommand*{\distiid}{\distas{\text{i.i.d}}}

\makeatletter
\def\moverlay{\mathpalette\mov@rlay}
\def\mov@rlay#1#2{\leavevmode\vtop{%
   \baselineskip\z@skip \lineskiplimit-\maxdimen
   \ialign{\hfil$\m@th#1##$\hfil\cr#2\crcr}}}
\newcommand*{\charfusion}[3][\mathord]{
  #1{\ifx#1\mathop\vphantom{#2}\fi\mathpalette\mov@rlay{#2\cr#3}}
  \ifx#1\mathop\expandafter\displaylimits\fi}
\makeatother
\newcommand*{\cupdot}{\charfusion[\mathbin]{\cup}{\cdot}}
\newcommand*{\bigcupdot}{\charfusion[\mathop]{\bigcup}{\cdot}}

\newcommand*{\mt}[1]{\text{\normalfont #1}}

\newtheorem{theorem}{Theorem}[section]
\newtheorem{theorem*}{Theorem}
\newtheorem{corollary}{Corollary}[section]
\newtheorem{proposition}{Proposition}[section]
\newtheorem{lemma}{Lemma}[section]

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{definition*}{Definition}
\newtheorem{example}{Example}[section]
\newtheorem*{properties}{Properties}

\newtheoremstyle{algodesc}{}{}{}{}{\bfseries}{.}{ }{}%
\theoremstyle{algodesc}
\newtheorem{algodesc}{Algorithm}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\begin{document}

\title{Taxi Trip Density Estimation in NYC\thanks{2016 Applied Statistics Qualifying Exam report.}}
\author{
    Roger Fan\footnote{\url{rogerfan@umich.edu}}
}
\date{May 26, 2016}

\maketitle


\section{Introduction}
Demand estimation is a crucial problem for taxicab companies (and potentially other ride-share services), allowing them to more effectively plan and adjust deployments. Using a dataset of New York City taxi trip data, we show how clustering and density estimation techniques can be applied to this problem. Although taxi trip originations are not perfect indicators of demand, as a completed trip requires both the demand for a trip and a taxi available to supply it, we use taxi origination locations to hopefully proxy for taxi demand.

We use a Gaussian mixture model (GMM) to estimate the density of trip origination locations in New York City. This method allows us to effectively condense the information from millions of taxi trip originations to a manageable number of Gaussian distributions over the city.

We expect, however, that the demand for taxis changes over time, a factor that is vital to many applications of this analysis. To better handle this issue, we design a extension to the standard Gaussian mixture model that allows for time-varying mixing weights and use an Expectation-Maximization (EM) algorithm to estimate it.


\section{Data}
The NYC Taxi and Limousine Commision (TLC) provides extensive data on taxicab trips in New York City for the last several years.\footnote{Available at \url{http://www.nyc.gov/html/tlc/html/about/trip_record_data.shtml}.} This data includes both yellow cabs, which primarily pick up street hails in Manhattan and at the airports, and green cabs, which can only be hailed in northern Manhattan and the outer boroughs.

The primary variables of interest are the time and location of each trip's pickup and dropoff. Location data is encoded as latitude and longitude, which we will be able to use directly, and time data is recorded to the second, though we will primarily be using data at the hourly frequency in this analysis.

To simplify estimation and computation, we will focus on taxi rides conducted during a single week, December 7-10, 2015. We omit Friday-Sunday of the week to avoid complicating the analysis with weekend data, and so are left with four days of weekday data, consisting of just over 1.7 million individual taxi rides. Of these, around 25 thousand are missing location data, which is a small enough proportion that it is safe to simply omit them. And finally, we omit around 100 outlier trips that have nonsensical or extreme pickup or dropoff locations, leaving a final training dataset of 1,688,673 taxi trips.


\section{Density Estimation}
We can consider approximating taxi demand to be a density estimation problem, where each trip's pickup location is a draw from an underlying distribution. Estimating taxi demand then simply becomes a problem of estimating the (2-dimensional) density of taxi originations.


\subsection{Gaussian Mixture Model} \label{sec:gmm}
In order to estimate the density, we will assume that joint density for $X$ comes from a $J$-component Gaussian mixture model and maximize the likelihood using the EM algorithm, which has been the standard estimation procedure for mixture models since \citet{dempsterlairdrubin77} first introduced the EM algorithm. We introduce a latent variable $Z$ that indicates group membership and have a generative model defined by
\begin{equation} \label{eq:gmm}
\begin{aligned}
Z &\dist \mt{Categorical}(\pi_1, \dots, \pi_J) \\
X \cond Z = j &\dist \mt{Gaussian}(\mu_j, \Sigma_j)
\end{aligned}
\end{equation}
The EM algorithm updates are therefore as follows.
\begin{enumerate}
\item
  E-step: Estimate the group probabilities $p_{ij}^{(t+1)}$ conditional on the current parameter estimates.
  \begin{equation} \label{eq:gmm_estep}
  p_{ij}^{(t+1)} = \frac{\pi_j^{(t)} P( X_i \cond \mu_j^{(t)}, \Sigma_j^{(t)})}{\sum_{j'=1}^J \pi_{j'}^{(t)} P( X_i \cond \mu_{j'}^{(t)}, \Sigma_{j'}^{(t)})}
  \end{equation}

\item
  M-step: Estimate the parameters using maximum likelihood estimators conditional on the group probability estimates.
  \begin{equation} \label{eq:gmm_mstep}
  \begin{aligned}
  \pi_j^{(t+1)} &= \frac{1}{n} \sum_{i=1}^n p_{ij}^{(t+1)} \\
  \mu_j^{(t+1)} &= \frac{\sum_{i=1}^n p_{ij}^{(t+1)} X_i}{\sum_{i=1}^n p_{ij}^{(t+1)}} \\
  \Sigma_j^{(t+1)} &= \frac{\sum_{i=1}^n p_{ij}^{(t+1)} (X_i - \mu_j^{(t+1)}) (X_i - \mu_j^{(t+1)})^T}{\sum_{i=1}^n p_{ij}^{(t+1)}}
  \end{aligned}
  \end{equation}
\end{enumerate}

\subsection{Time-Varying Mixing Components} \label{sec:gmm_cat}
However, the model described in Equation~\ref{eq:gmm} has a major weakness: it does not use temporal information. We expect that the demand for taxis could vary significantly over different time periods. To illustrate this, Figure~\ref{fig:diff} plots taxi originations for two time periods, 7am-9am and 10am-12am.\footnote{These subsamples have approximately the same number of observations.} Even using just the raw data, we can visually identify several areas that exhibit differences. In particular, the morning taxi demand seems to be higher in southwest Brooklyn (-74.00, 40.68), northern Queens (-73.92, 40.76), the Bronx (-73.90, 40.82), and northern Manhattan (-73.95, 40.81), while nighttime demand is higher in Williamsburg in northern Brooklyn (-73.94, 40.71). The morning periods in particular are as we might expect, all communities that commute into the city during morning rush hour.

\begin{figure}[htbp] \centering
  \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\linewidth]{./include/diff_morningrush.png}
    \caption{7am-9am} \label{fig:diff:morning}
  \end{subfigure}
  \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\linewidth]{./include/diff_night.png}
    \caption{10pm-12am} \label{fig:diff:night}
  \end{subfigure}
  \caption{Taxi originations during morning rush hour (7am-9am) and at night (10pm-12am).}
  \label{fig:diff}
\end{figure}

This motivates extending the Gaussian mixture model to incorporate temporal information. We will assume that the component distributions are the same over time, as if clusters are approximating neighborhoods then it seems reasonable to assume that the location and shape of each cluster is constant. But we will allow the mixing proportions to vary between time periods, allowing the demand from each neighborhood to shift over time. This, for instance, allows for phenomenon such as commuter neighborhoods that have high demand during rush hour but little during the evening. It will also hopefully allow us to identify clusters that are only visible during specific time periods.

To account for this, we use an additional observed variable $Y \in \{1, \dots, K\}$ that tracks the time period of each obervation $X$. We will assume that $Y$ is fixed and exogenous. Then our model becomes
\begin{equation} \label{eq:gmm_cat}
\begin{aligned}
Z \cond Y = k &\dist \mt{Categorical}(\pi_{k1}, \dots, \pi_{kJ}) \\
X \cond Z = j &\dist \mt{Gaussian}(\mu_j, \Sigma_j)
\end{aligned}
\end{equation}
The EM algorithm is similar to the one described in Section~\ref{sec:gmm}, replacing the group probability update in Equation~\ref{eq:gmm_estep} with
\begin{equation} \label{eq:gmm_cat_estep}
p_{ij}^{(t+1)} = \frac{\pi_{Y_i j}^{(t)} P( X_i \cond \mu_j^{(t)}, \Sigma_j^{(t)})}{\sum_{j'=1}^J \pi_{Y_i j'}^{(t)} P( X_i \cond \mu_{j'}^{(t)}, \Sigma_{j'}^{(t)})}
\end{equation}
And replacing the mixing component update in Equation~\ref{eq:gmm_mstep} with
\begin{equation} \label{eq:gmm_cat_mstep}
\pi_{kj}^{(t+1)} = \frac{\sum_{i=1}^n \I(Y_i = k) p_{ij}^{(t+1)} }{\sum_{i=1}^n \I(Y_i = k)}
\end{equation}

Note that the advantage of dividing time into a (small) number of categories and estimating each category's mixing components separately is that it does not add much computational complexity over the standard GMM model. A richer model might be to assume the mixing proportion for each cluster varies smoothly over time and then use a local averaging or kernel regression method instead of Equation~\ref{eq:gmm_cat_mstep}, but this would add a significant computational burden.


\section{Results}
Figure~\ref{fig:gmm_res} shows the estimated density of the basic GMM model (Equation~\ref{eq:gmm}) with $J=30$ components as well as each of the cluster means. We can see that the contours of the estimated density visually correspond well to the raw origination data. Several of the important features of NYC are identifiable, including the overall shape and size of the outer boroughs, the locations of JFK and LaGuardia airports (the two hotspots to the east), and the overall shape of Manhattan and Midtown, including a cool spot in Central Park. The red centers indicate the three clusters with the highest estimated mixing proportions, which together make up around 37\% of originations.

\begin{figure}[tb] \centering
  \includegraphics[width=0.8\linewidth]{./include/gmm_res.png}
  \caption{Estimated cluster centers and density of NYC taxi originations using a Gaussian mixture model with $J=30$ components.}
  \label{fig:gmm_res}
\end{figure}

Though this is effective for summarizing and visualizing the data and captures many of the relevant features of the data and city, it is not particularly effective for planning deployments or estimating demand since it does not allow for time-dependent predictions. Therefore, our next step is to estimate a model that incorporates temporal information.

To estimate the GMM model with time-varying mixing proportions described in Equation~\ref{eq:gmm_cat}, we first need to divide time into categories. Attempting to adhere to common-sense work day divisions, we propose using six categories: early morning (2am-7am), morning rush hour (7am-9am), work day (9am-4pm), evening rush hour (4pm-6pm), evening (6pm-10pm), and night (10pm-2am). Figure~\ref{fig:time} shows the frequency of taxi pickups over time as well as these proposed categories. We can see that the categories seem to correspond well to natural divisions in data, with boundaries that roughly track change points in the frequency over time.

\begin{figure}[tb] \centering
  \includegraphics[width=0.8\linewidth]{./include/time.pdf}
  \caption{Taxi pickups over time with proposed time categories.}
  \label{fig:time}
\end{figure}

Figure~\ref{fig:gmm_cat_res} shows the estimated centers for this model. We can see that the centers outside of Manhattan are nearly identical to those in Figure~\ref{fig:gmm_res}, but that the centers inside Manhattan are fairly different. It is notable that the estimated component distributions are noticeably different than the basic GMM model, hopefully adding time-varying mixing proportions is allowing the model to identify previously difficult-to-separate components.

\begin{figure}[tb] \centering
  \includegraphics[width=0.7\linewidth]{./include/gmm_cat_res.png}
  \caption{Estimated cluster centers of NYC taxi originations using a Gaussian mixture model with time-varying mixing proportions. $K=30$ components and $K=6$ time categories are used.}
  \label{fig:gmm_cat_res}
\end{figure}

Table~\ref{tab:mixing} shows the evolution of mixing proportions for a subset of the clusters. We can see that there is significant variation across time for many of the clusters, and that the patterns can also be very different. JFK (cluster 5) and LaGuardia (cluster 6) have similar patterns over time except for in the early morning, where JFK has a relatively high percentage of taxi originations and LaGuardia has almost none. Or consider southwest Brooklyn (cluster 7), which is primarily busy during rush hour and the work day, compared to Williamsburg (cluster 8), which has more originations at night and in the early morning and very few during the day. Clusters in Mahattan can also be very different, as the Lower East Side (cluster 1) seems to be a night-life area, while the Upper East Side (cluster 0) has relatively few originations at night. Also note that cluster 1 is one of those that the basic GMM model was unable to find; it's unique temporal pattern allows this model to identify it.

\begin{table}[H] \centering
\begin{tabular}{rrrrrrrrrrr}
  \toprule
   & \multicolumn{5}{c}{Manhattan} & \multicolumn{2}{c}{Airports} & \multicolumn{3}{c}{Boroughs} \\
   \cmidrule(lr){2-6} \cmidrule(lr){7-8} \cmidrule(lr){9-11}
   & \multicolumn{1}{c}{0} & \multicolumn{1}{c}{1} & \multicolumn{1}{c}{2} & \multicolumn{1}{c}{3} & \multicolumn{1}{c}{4} & \multicolumn{1}{c}{5} & \multicolumn{1}{c}{6} & \multicolumn{1}{c}{7} & \multicolumn{1}{c}{8} & \multicolumn{1}{c}{9} \\
  \midrule
  02:00-07:00 &  8.10 &  5.72 &  8.97 & 14.38 & 17.72 &  2.09 &  0.16 &  1.18 &  1.45 &  1.62 \\
  07:00-09:00 &  8.53 &  1.82 & 11.91 &  8.56 & 18.40 &  1.27 &  1.87 &  1.86 &  0.23 &  1.58 \\
  09:00-16:00 &  7.19 &  1.83 & 11.95 &  6.77 & 18.91 &  1.72 &  3.58 &  1.20 &  0.25 &  1.23  \\
  16:00-18:00 &  8.06 &  1.68 &  9.86 &  5.64 & 15.20 &  2.64 &  3.73 &  1.54 &  0.36 &  1.59 \\
  18:00-22:00 &  6.78 &  3.76 & 12.51 &  7.14 & 17.64 &  1.75 &  2.54 &  1.30 &  0.73 &  1.71 \\
  22:00-02:00 &  4.19 &  8.61 & 12.23 &  8.55 & 18.02 &  1.69 &  1.96 &  0.97 &  2.04 &  1.92 \\
  \bottomrule
\end{tabular}
\caption{Estimated mixing proportions for a subset of clusters.}
\label{tab:mixing}
\end{table}

Figure~\ref{fig:dist} compares the estimated distributions for morning rush hour and late-night. We can see that many of the same patterns visible in Figure~\ref{fig:diff} are also clear here, where the northern and southern boroughs have more originations in the morning while Williamsburg has more originations at night. But using the estimated distributions, we can also identify patterns in Manhattan that were impossible to see with the raw data. For instance, we can clearly see the nighttime hotspot in southern Manhattan that is the Lower East Side, and we can see how northern Manhattan and the Upper East Side have fewer night originations.

\begin{figure}[htbp] \centering
  \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\linewidth]{./include/gmm_cat_morn.png}
    \caption{7am-9am} \label{fig:dist:morning}
  \end{subfigure}
  \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\linewidth]{./include/gmm_cat_night.png}
    \caption{10pm-2am} \label{fig:dist:night}
  \end{subfigure}
  \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\linewidth]{./include/gmm_cat_morn2.png}
    \caption{7am-9am} \label{fig:dist:morning2}
  \end{subfigure}
  \begin{subfigure}[t]{.49\linewidth}
    \includegraphics[width=\linewidth]{./include/gmm_cat_night2.png}
    \caption{10pm-2am} \label{fig:dist:night2}
  \end{subfigure}
  \caption{Estimated trip origination densities for two time periods using GMM with time-varying mixing proportions.}
  \label{fig:dist}
\end{figure}


\subsection{Evaluating Fit}
In order to evaluate the fit of these models, we use a test dataset consisting of 426,194 observations from December 15, 2015. We consider three models. The first is the standard GMM model described in Equation~\ref{eq:gmm}. The second, which we call subset-GMM (ssGMM), is the same GMM model estimated separately for each of the six time category subsets. And the third is the GMM model with time-varying mixing proportions (tvGMM) as described in Equation~\ref{eq:gmm_cat}. Note that GMM is nested inside tvGMM which is in turn nested in ssGMM.

Table~\ref{tab:logliks} presents diagnostics for these three models. For each model, we calculate the log-likelihood contribution for each time period of the test data, as well as the total test log-likelihood. We also calculate the Bayesian information criterion (BIC) for each model on the training data to evaluate in-sample performance.\footnote{BIC is calculated as $p \log n - 2 \log L$, where $p$ is the number of estimated parameters, $n$ is the number of observations, and $L$ is the maximized likelihood of the model. Lower BICs indicate more parsimonious models.}

\begin{table}[htb] \centering
\begin{tabular}{rrrr}
  \toprule
   & \multicolumn{1}{c}{GMM} & \multicolumn{1}{c}{ssGMM} & \multicolumn{1}{c}{tvGMM} \\
  \midrule
  02:00-07:00 &  125126 &  \textbf{128015} &  127873 \\
  07:00-09:00 &  235173 &  236006 &  \textbf{237738} \\
  09:00-16:00 &  773825 &  769287 &  \textbf{780562} \\
  16:00-18:00 &  211914 &  210956 &  \textbf{214326} \\
  18:00-22:00 &  601819 &  600960 &  \textbf{606997} \\
  22:00-02:00 &  292844 &  296174 &  \textbf{298605} \\
  \midrule
  Total log-lik &  2240702 &  2241399 &  \textbf{2266101} \\
  In-Sample BIC & -17732572 & -17732375 & \textbf{-17934791} \\
            % BIC & -4479083 & -4468876 & \textbf{-4528002} \\
  \bottomrule
\end{tabular}
\caption{Out-of-sample log-likelihoods and in-sample BIC.}
\label{tab:logliks}
\end{table}

We can see that, overall, tvGMM has significantly better out-of-sample fit than either GMM or ssGMM. In fact, tvGMM outperforms ssGMM and GMM on all but one of the subsets considered. ssGMM and GMM have very similar out-of-sample performance in terms of log-likelihood. In terms of BIC, tvGMM again clearly has the best performance, with GMM and ssGMM performing similarly. It seems that the penalty for the many more free parameters in ssGMM outweighs any performance gains it might have.

tvGMM seems to provide a much more effective and parsimonious way allow for variation across time periods while sharing information when appropriate. It performs significantly better than both GMM and ssGMM in terms of in-sample and out-of-sample diagnostics.


\section{Conclusion}
In an attempt to proxy for taxi demand in NYC, we have used data on taxi trip origination locations and times to conduct density estimation using Gaussian mixture models. Standard GMMs can effectively estimate the underlying density of taxi originations and recover features of the city and surrouding boroughs, but do not incorporate time, which is an essential factor for many possible applications of this analysis. In order to remedy this, we design a computationally feasible extension to the GMM that allows for time-varying mixing proportions and present a modified EM algorithm to estimate this model. This model effectively recovers temporal patterns in the density and both its in-sample and out-of-sample performance are superior to both the standard GMM and separate GMMs estimated on each subset.

One improvement to the analysis would be to use data-driven methods to determine the hyperparameters. Due to computational concerns, hyperparameters such as the number of clusters and the time categories were simply chosen, instead of being optimized. Incorporating model selection based on cross-validation, AIC/BIC, or similar techniques could improve performance.

As briefly mentioned in Section~\ref{sec:gmm_cat}, a possible extension to this model is to allow for the mixing proportions to be an arbitrary smooth function of time instead of effectively a step function. For some smooth functions $\pi_1, \dots, \pi_J$, the corresponding model is then described by:
\begin{equation} \label{eq:gmm_smooth}
\begin{aligned}
Z \cond Y = y &\dist \mt{Categorical}(\pi_{1}(y), \dots, \pi_{J}(y)) \\
X \cond Z = j &\dist \mt{Gaussian}(\mu_j, \Sigma_j)
\end{aligned}
\end{equation}
With a modified EM algorithm where the probability estimates in the M-step are replaced with Nadaraya-Watson estimators \citep[as described in Chapter 6 of][]{esl}.
\begin{equation}
\pi^{(t+1)}_j(y) = \frac{\sum_{i=1}^n K(y, Y_i) p_{ij}^{(t+1)}}{\sum_{i=1}^n K(y, Y_i)}
\end{equation}
For some kernel function $K$ (other non- or semi-parametric regression estimates could also likely be used). This is a much richer model that can estimate complicated temporal patterns, and applications like planning taxi deployment would likely benefit from the finer temporal resolution. However, the regression step within each EM iteration makes this algorithm computationally difficult when applied to large datasets such as this one.

Another weakness of GMM-based models is that New York is a city with strong structure, where possible pickup locations are restricted by various parks, waterways, highways, legal boundaries, etc. Gaussian distributions are not well-suited to estimating these kinds of hard boundaries. Kernel density estimators with kernel warping such as those proposed by \citet{zhoumatteson} could be used to simultaneously respect these structures and share information over time, but doing so in a way that scales computationally could be challenging.

Finally, when it comes to applications like taxi deployment, the number of trips is not the only concern. Factors such as trip destination, length, and overall fares are all potentially important as well, and data on all these factors is also provided by the TLC. Incorporating this additional information would be important to move from simply modeling demand to modeling potential revenue or profit.


\bibliographystyle{apa}
\bibliography{biblio}


\end{document}
